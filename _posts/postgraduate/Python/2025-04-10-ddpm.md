```
layout: post
title: "DDPM"
date: 2025-04-10 09:00:00 +0800 
categories: ç ”ç©¶ç”Ÿæ¶¯
tag: Python
```



# ðŸ§  DDPM

[TOC]





## ðŸ”„ ä¸€ã€æ ¸å¿ƒæ€æƒ³ï¼šä»Žå™ªå£°åˆ°å›¾åƒ

æ‰©æ•£æ¨¡åž‹æ˜¯ä¸€ç§**ç”Ÿæˆæ¨¡åž‹**ï¼Œç›®æ ‡æ˜¯ä»Žçº¯é«˜æ–¯å™ªå£°ä¸€æ­¥æ­¥**ç”ŸæˆçœŸå®žå›¾åƒ**ã€‚

å®ƒåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š

| é˜¶æ®µ | æ–¹å‘          | åç§°                | åšäº†ä»€ä¹ˆ                     |
| ---- | ------------- | ------------------- | ---------------------------- |
| æ­£å‘ | $x_0 \to x_T$ | Forward / Diffusion | ä¸æ–­åŠ å™ªï¼Œè®©å›¾åƒå˜æˆéšæœºå™ªå£° |
| åå‘ | $x_T \to x_0$ | Reverse / Denoising | å­¦ä¹ åŽ»å™ªï¼Œè¿˜åŽŸå‡ºåŽŸå§‹å›¾åƒ     |

------

## ðŸ“¦ äºŒã€æ­£å‘è¿‡ç¨‹ï¼šåŠ å™ª

æˆ‘ä»¬ä»ŽåŽŸå›¾ $x_0$ å‡ºå‘ï¼Œåœ¨æ¯ä¸ªæ—¶åˆ» $t$ åŠ å…¥ä¸€ç‚¹å™ªå£°ï¼Œæœ€ç»ˆå¾—åˆ° $x_T$ï¼Œä¸€ä¸ªè¿‘ä¼¼é«˜æ–¯å™ªå£°çš„å›¾åƒï¼š
$$
x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

- $\bar{\alpha}_{t}=\prod_{i=1}^{t} \alpha_{i},$  å…¶ä¸­ $\alpha_{i}=1-\beta_{i}$ ($Î²_i$ æ˜¯ç¬¬ $i$ æ­¥åŠ çš„å™ªå£°å¼ºåº¦), ä»Žç¬¬ 1 æ­¥åˆ°ç¬¬ t æ­¥ç´¯è®¡ä¿ç•™çš„å›¾åƒä¿¡æ¯é‡
- $t = 0 \to T$ï¼Œå›¾åƒè¶Šæ¥è¶Šæ¨¡ç³Šï¼›
- è¿™ä¸ªè¿‡ç¨‹æ˜¯å¯é—­å¼è®¡ç®—çš„ï¼Œæ— éœ€ä¸€æ­¥æ­¥æ‰§è¡Œï¼Œ**ä¸€æ¬¡å…¬å¼å°±èƒ½ç”Ÿæˆ $x_t$â€‹ âœ…**

------

## ðŸ§  ä¸‰ã€åå‘è¿‡ç¨‹ï¼šå­¦ä¹ åŽ»å™ª 

### ðŸŽ¯ ç›®æ ‡

ä»Ž $x_T \sim \mathcal{N}(0, I)$ å‡ºå‘ï¼Œ**é€æ­¥åŽ»å™ªè¿˜åŽŸå‡º $x_0$**ã€‚

### ðŸ¤– å­¦ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªç¥žç»ç½‘ç»œ $\epsilon_\theta(x_t, t)$ æ¥é¢„æµ‹=é¢„æµ‹åŠ å™ªæ—¶ç”¨çš„ $\epsilon$ã€‚

------

### ðŸ” Trickï¼šä»Ž $x_t$ æŽ¨å‡º $x_0$ï¼Œå†æŽ¨å‡º $x_{t-1}$

ä»Žæ­£å‘å…¬å¼ï¼š
$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon
$$


å¯ä»¥åè§£å‡ºï¼š
$$
x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_\theta(x_t, t) \right)
$$


âœ… æœ‰äº† $\epsilon_\theta$ï¼Œå°±èƒ½ä¼°è®¡ $x_0$ï¼ŒæŽ¥ç€ç»§ç»­è®¡ç®— $x_{t-1}$ã€‚

------

### â“ä¸ºä»€ä¹ˆä¸ç›´æŽ¥ä»Ž $x_t$ ç®— $x_{t-1}$ï¼Ÿ

è™½ç„¶æ­£å‘æœ‰ï¼š
$$
x_t = f(x_{t-1}) + \text{noise}
$$
ä½†åå‘æ˜¯**æ¦‚çŽ‡åˆ†å¸ƒ**ï¼Œä¸æ˜¯å‡½æ•°ã€‚å› ä¸ºï¼š

- å¤šä¸ª $x_{t-1}$ å¯èƒ½åŠ ä¸Šä¸åŒå™ªå£°åŽå˜æˆåŒä¸€ä¸ª $x_t$
- æ‰€ä»¥ $q(x_{t-1} | x_t)$â€‹ æ˜¯ä¸ªå¤æ‚åˆ†å¸ƒï¼Œæ²¡æ³•æ˜¾å¼è¡¨ç¤º

æˆ‘ä»¬æ²¡æœ‰åŠžæ³•å¾—åˆ° $q(x_{t-1})$æˆ– $q(x_t)$çš„æ˜Žç¡®è¡¨è¾¾å¼ï¼Œå› ä¸ºå®ƒä»¬æ¶‰åŠä»Ž $x_0$ ç§¯åˆ†è¿‡æ¥çš„æ‰€æœ‰è·¯å¾„ï¼š
$$
q(x_t) = \int q(x_t \mid x_{t-1}) q(x_{t-1}) dx_{t-1}
$$
ä½† $q(x_{t-1})$å¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒï¼å› ä¸ºå®ƒæœ¬èº«æ˜¯ä»Žä¸€ç³»åˆ—æœ‰å™ªå£°æ‰°åŠ¨çš„æ­¥éª¤ä¸­**ä¸€æ­¥æ­¥å·ç§¯å‡ºæ¥çš„å¤æ‚åˆ†å¸ƒ**ï¼Œç„¶åŽ $q(x_{t-2})$è¿˜è¦å†ç”± $q(x_{t-3})$ æŽ¨æ¥â€¦â€¦æœ€ç»ˆéƒ½ä¾èµ–äºŽ $q(x_0)$ï¼Œä¹Ÿå°±æ˜¯**åŽŸå§‹æ•°æ®åˆ†å¸ƒ**ã€‚ä½†ï¼ðŸ’¥ **æˆ‘ä»¬æ ¹æœ¬ä¸çŸ¥é“ $q(x_0)$** æ˜¯ä»€ä¹ˆï¼

å› æ­¤ï¼Œæˆ‘ä»¬åªèƒ½é€šè¿‡$x_0$ä¼°è®¡å®ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚

------

## ðŸ” åå‘é‡‡æ ·å…¬å¼ä¼°è®¡ï¼Œå¼•å…¥$x_0$ï¼Œç”¨å¯è§£çš„ $q(x_{t-1} \mid x_t, x_0)$ æ¥ä¼°è®¡$x_{t-1}$

åˆ©ç”¨è´å¶æ–¯å…¬å¼ï¼š
$$
p\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, x_{0}\right)=\frac{p\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{t-1}\right) p\left(\boldsymbol{x}_{t-1} \mid x_{0}\right)}{p\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}\right)}
$$
å¼å­ä¸­æ¯ä¸€é¡¹éƒ½æ˜¯å¯è§£çš„é«˜æ–¯åˆ†å¸ƒï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç”¨æ¡ä»¶é«˜æ–¯ä¹˜ç§¯å…¬å¼ï¼Œå¾—åˆ°ï¼š
$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(\mu_\theta(x_t, t), \sigma_t^2 I)
$$
å…¶ä¸­ï¼š

**å‡å€¼**ï¼š
$$
\mu_\theta = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \epsilon_\theta(x_t, t) \right)
$$
ä¹Ÿå¯ä»¥å†™æˆåŸºäºŽ `xâ‚€` çš„å½¢å¼ï¼š
$$
\mu\left(x_{t}, x_{0}\right)=\frac{\sqrt{\bar{\alpha}_{t-1}} \cdot \beta_{t}}{1-\bar{\alpha}_{t}} x_{0}+\frac{\sqrt{1-\beta_{t}} \cdot\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} x_{t}
$$


**æ–¹å·®**ï¼š

- **é€‰æ‹©**ï¼šæ–¹å·®$\sigma_t^2$è®¾ä¸ºå¸¸æ•°ï¼ˆä¸è®­ç»ƒï¼‰ï¼Œå®žéªŒå‘çŽ°ä¸¤ç§é€‰æ‹©æ•ˆæžœç›¸ä¼¼ï¼š
  - $\sigma_t^2 = \beta_t$ï¼ˆå¯¹åº”æ•°æ®åˆå§‹ä¸ºé«˜æ–¯åˆ†å¸ƒï¼‰
  - $\sigma_t^2 = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t$ï¼ˆå¯¹åº”æ•°æ®åˆå§‹ä¸ºå•ç‚¹åˆ†å¸ƒï¼‰

------

### ðŸ¤” å¸¸è§é—®é¢˜è§£æž

**Q1ï¼šä¸ºå•¥ä¸ç›´æŽ¥è®­ç»ƒä¸€ä¸ªå­¦ $x_{t-1}$çš„æ¨¡åž‹ï¼Ÿ**

- ç©ºé—´å¤ªå¤§ï¼Œä¸å®¹æ˜“æ”¶æ•›ï¼›
- æ²¡æ³•ç›´æŽ¥ç›‘ç£ $x_{t-1}$ï¼Œä½†èƒ½ç›‘ç£ $\epsilon$

**Q2ï¼šä¸ºä»€ä¹ˆä¸ç›´æŽ¥ç”¨é¢„æµ‹çš„ $x_0$ å½“ä½œæœ€ç»ˆçš„ç”Ÿæˆç»“æžœï¼Ÿ**

- é¢„æµ‹çš„ $x_0$ æ˜¯è¿‘ä¼¼å€¼ï¼›
- å¤šä¸ª $x_t$ æŽ¨å‡ºçš„ $x_0$ ä¸ä¸€è‡´ï¼›
- æ‰©æ•£æ¨¡åž‹æœ¬è´¨æ˜¯ä¸€æ­¥æ­¥å‡€åŒ–ï¼Œä¸èƒ½ä¸€æ­¥åˆ°ä½ã€‚

**Q3ï¼šä¸ºä»€ä¹ˆç”¨ UNet é¢„æµ‹å™ªå£° Ïµï¼Œè€Œä¸æ˜¯ç›´æŽ¥é¢„æµ‹çœŸå®žåå‘å‡å€¼?**

- å› ä¸ºå™ªå£° Ïµ çš„åˆ†å¸ƒå›ºå®šï¼Œé¢„æµ‹æ›´å®¹æ˜“ï¼Œè®­ç»ƒæ›´ç¨³å®šï¼›
- é€šè¿‡æ•°å­¦æŽ¨å¯¼ï¼ˆå¼10ï¼‰ï¼Œå‘çŽ°å¯ä»¥æ”¹å†™ä¸ºé¢„æµ‹å™ªå£°Ïµçš„å½¢å¼ï¼Œè®¡ç®—æ›´ç®€å•

------

## ðŸ‹ï¸â€â™€ï¸ æ¨¡åž‹è®­ç»ƒ

è®­ç»ƒæ—¶ä¼˜åŒ–ï¼š
$$
\mathcal{L}_{\text{simple}} = \mathbb{E}_{x_0, t, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right]
$$


æµç¨‹å¦‚ä¸‹ï¼š

1. ä»ŽçœŸå®žå›¾åƒ $x_0$ é‡‡æ · $t$ï¼ŒåŠ å™ªå¾— $x_t$
2. ç”¨ç½‘ç»œé¢„æµ‹å™ªå£° $\epsilon_\theta(x_t, t)$
3. ç”¨MSEè®¡ç®—æŸå¤±ï¼Œä¸ŽçœŸå®ž $\epsilon$ å¯¹æ¯”

------

## ðŸŽ¨ æŽ¨ç† / é‡‡æ ·é˜¶æ®µ

ä»Žé«˜æ–¯å™ªå£° $x_T$ å¼€å§‹ï¼ŒæŒ‰å¦‚ä¸‹å…¬å¼é€æ­¥é‡‡æ ·ç›´åˆ° $x_0$ï¼š
$$
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t \cdot z, \quad z \sim \mathcal{N}(0, I)
$$
æ¯æ­¥é€»è¾‘ï¼š

- å…ˆé¢„æµ‹å™ªå£° $\epsilon_\theta$
- å†ä¼°è®¡ $x_0$ï¼ŒæŽ¨å¯¼ $\mu_\theta$
- åŠ å…¥éšæœºå™ªå£° $z$ å¾—åˆ° $x_{t-1}$
- ä¸æ–­é‡å¤ï¼Œæœ€ç»ˆå¾—åˆ°ç”Ÿæˆå›¾åƒï¼





## ðŸ§© Diffusion æ¨¡åž‹æ¨¡æ¿ä»£ç 

å‚è€ƒpytorchä»£ç ï¼š[GitHub - chunyu-li/ddpm: æ‰©æ•£æ¨¡åž‹çš„ç®€æ˜“ PyTorch å®žçŽ°](https://github.com/chunyu-li/ddpm)

### 1. åˆå§‹åŒ–å’Œå¿…è¦çš„å¯¼å…¥

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torchvision
from torchvision import transforms
import numpy as np
import random
```

### 2. å®šä¹‰ Beta Schedule å’Œç›¸å…³å‡½æ•°

```python
# çº¿æ€§ beta scheduleï¼ˆæŽ§åˆ¶æ¯æ­¥å™ªå£°çš„å¤§å°ï¼‰
def linear_beta_schedule(timesteps, start=0.0001, end=0.02):
    return torch.linspace(start, end, timesteps)

# èŽ·å–ç´¯ç§¯çš„ alpha å€¼
def get_alphas(betas):
    return 1.0 - betas

# èŽ·å–ç´¯ç§¯ alpha çš„ä¹˜ç§¯
def get_alphas_cumprod(alphas):
    return torch.cumprod(alphas, axis=0)

# è®¡ç®—åå‘å™ªå£°çš„æ ‡å‡†å·®
def get_posterior_variance(alphas_cumprod, alphas_cumprod_prev, betas):
    return betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
```

### 3. å®šä¹‰ Diffusion æ¨¡åž‹ï¼ˆUNetï¼‰

```python
class SimpleUnet(nn.Module):
    def __init__(self):
        super(SimpleUnet, self).__init__()
        # è¿™é‡Œå®šä¹‰ä¸€ä¸ªç®€å•çš„å·ç§¯ç½‘ç»œä½œä¸ºç¤ºä¾‹ï¼Œå¯ä»¥æ›¿æ¢æˆæ›´å¤æ‚çš„UNet
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(128 * 64 * 64, 256)
        self.fc2 = nn.Linear(256, 3 * 64 * 64)

    def forward(self, x, t):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # Flatten for fully connected layers
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x.view(x.size(0), 3, 64, 64)  # Reshape back to image shape
```

### 4. æ‰©æ•£è¿‡ç¨‹å’ŒåŽ»å™ªè¿‡ç¨‹

#### æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼šä»Ž $x_0$ åˆ° $x_t$

```python
def forward_diffusion_sample(x_0, t, betas, alphas_cumprod):
    noise = torch.randn_like(x_0)
    sqrt_alphas_cumprod_t = alphas_cumprod[t].view(-1, 1, 1, 1)  # å¹¿æ’­è‡³æ‰¹æ¬¡ç»´åº¦
    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t]).view(-1, 1, 1, 1)
    
    x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise
    return x_t, noise
```

#### åå‘æ‰©æ•£è¿‡ç¨‹ï¼šä»Ž $x_T$ åˆ° $x_0$

```python
def sample_timestep(x_t, t, model, alphas_cumprod, betas):
    # æ¨¡åž‹é¢„æµ‹å™ªå£°
    epsilon_pred = model(x_t, t)

    # è®¡ç®—å½“å‰æ—¶åˆ»çš„å‡å€¼å’Œæ ‡å‡†å·®
    sqrt_alphas_cumprod_t = alphas_cumprod[t].view(-1, 1, 1, 1)
    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t]).view(-1, 1, 1, 1)
    posterior_variance_t = betas[t].view(-1, 1, 1, 1)

    # è®¡ç®—é¢„æµ‹çš„x_0
    x_0_pred = (x_t - sqrt_one_minus_alphas_cumprod_t * epsilon_pred) / sqrt_alphas_cumprod_t

    # åå‘é‡‡æ ·
    noise = torch.randn_like(x_t) if t > 0 else torch.zeros_like(x_t)
    x_t_minus_1 = sqrt_alphas_cumprod[t - 1] * x_0_pred + sqrt_one_minus_alphas_cumprod[t - 1] * noise
    return x_t_minus_1
```

### 5. æŸå¤±å‡½æ•°ï¼ˆè®­ç»ƒæ—¶ï¼‰

```python
def get_loss(model, x_0, t, betas, alphas_cumprod):
    x_t, noise = forward_diffusion_sample(x_0, t, betas, alphas_cumprod)
    noise_pred = model(x_t, t)
    return F.mse_loss(noise, noise_pred)  # MSE æŸå¤±
```

### 6. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†

```python
def load_transformed_dataset(img_size=64, batch_size=128):
    data_transforms = [
        transforms.Resize((img_size, img_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Lambda(lambda t: (t * 2) - 1),  # [0,1] -> [-1,1]
    ]
    data_transform = transforms.Compose(data_transforms)

    train = torchvision.datasets.ImageFolder(root="./stanford_cars/cars_train", transform=data_transform)
    test = torchvision.datasets.ImageFolder(root="./stanford_cars/cars_test", transform=data_transform)

    dataset = torch.utils.data.ConcatDataset([train, test])
    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
```

### 7. è®­ç»ƒå¾ªçŽ¯

```python
if __name__ == "__main__":
    # åˆå§‹åŒ–
    model = SimpleUnet()
    T = 300  # æ‰©æ•£æ­¥æ•°
    betas = linear_beta_schedule(T)
    alphas = get_alphas(betas)
    alphas_cumprod = get_alphas_cumprod(alphas)

    BATCH_SIZE = 128
    epochs = 100

    dataloader = load_transformed_dataset(batch_size=BATCH_SIZE)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        for batch_idx, (batch, _) in enumerate(dataloader):
            optimizer.zero_grad()
            batch = batch.to(device)

            t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()
            loss = get_loss(model, batch, t, betas, alphas_cumprod)
            loss.backward()
            optimizer.step()

            if batch_idx % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}")
```

### 8. é‡‡æ ·ï¼ˆç”Ÿæˆå›¾åƒï¼‰

```python
def generate_samples(model, T=300):
    # ä»Žå™ªå£°å¼€å§‹
    x_t = torch.randn((BATCH_SIZE, 3, 64, 64)).to(device)
    
    for t in reversed(range(T)):
        x_t = sample_timestep(x_t, t, model, alphas_cumprod, betas)
    
    return x_t
```

### 9. æ˜¾ç¤ºå›¾åƒ

```python
def show_tensor_image(image):
    image = image.squeeze().cpu().numpy().transpose(1, 2, 0)
    image = (image + 1.0) / 2.0  # [-1, 1] -> [0, 1]
    plt.imshow(image)
    plt.axis('off')
    plt.show()
```

------

